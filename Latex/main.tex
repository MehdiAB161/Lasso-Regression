\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
%\usepackage{layout}
%\usepackage{geometry}
%\usepackage{setspace}
\usepackage{soul}
\usepackage{ulem}
\usepackage{amsmath}
%\usepackage{eurosym}
%\usepackage{bookman}
%\usepackage{charter}
%\usepackage{newcent}
%\usepackage{lmodern}
%\usepackage{mathpazo}
%\usepackage{mathptmx}
%\usepackage{url}
%\usepackage{verbatim}
%\usepackage{moreverb}
%\usepackage{listings}
%\usepackage{fancyhdr}
%\usepackage{wrapfig}
%\usepackage{color}
%\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{amsthm}
%\usepackage{makeidx}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert^{2}}


\begin{document}


%---------------------------------------------------------------------------------
% PAGE DE GARDE
%---------------------------------------------------------------------------------

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % lignes horizontales
\setlength{\topmargin}{0in}
\center % Center everything on the page
 
%----------------------------------
% Logos
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\hspace*{-0.5cm}
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.5\textwidth}
\begin{flushright} \large
\hspace*{1.5cm}
\end{flushright}
\end{minipage}\\[1cm]

%-----------------------------------
% Section d'entête
\textsc{\Large École Nationale de la Statistique et de l'Administration Économique}\\[1.5cm] 
\textsc{{\LARGE Rapport du Projet de Statistique}\\~\\Janvier 2017}\\[1cm]

%-----------------------------------
% Section de titre
\HRule \\[1cm]
{ \huge \bfseries Introduction à la régression pénalisée et à l'estimateur lasso}\\[0.4cm] % Titre
\HRule \\[1cm]
 
%------------------------------------
% Section des auteurs
\begin{minipage}{0.5\textwidth}
\begin{flushleft} \large
\emph{Auteurs:}\\
Mehdi \textsc{Abbana Bennani} \\
Julien \textsc{Mattei} \\
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Superviseur:} \\
Edwin \textsc{Grappin} \\[0.5cm] 
\end{flushright}
\end{minipage}\\[1.2cm]

%-------------------------------------
% Section de date
{\large \today}\\[0.5cm] % Date, change the \today to a set date if you want to be precise

\vfill % Fill the rest of the page with whitespace
\end{titlepage}

%---------------------------------------------------------------------------------
%  FIN PAGE DE GARDE
%---------------------------------------------------------------------------------





\title{	}


\section{Régression OLS}


\paragraph{Question 1 :}

\begin{align}
&\underset{\beta}{min}\norm{ y - X \beta} \tag{On minimise l'erreur quadratique}
\\ &\nabla_{\beta} \norm{ y - X \beta}(\beta^{\star}) = 0 \tag{Le gradient est nul en $\beta^\star$}
\notag \\ &\nabla_{\beta} ( y - X \beta)^{T} ( y - X \beta) (\beta^{\star}) = 0
\notag \\ &\boxed{\beta^{\star} = (X^{T}X)^{-1}X^{T}y}
\end{align}


Dans la suite on notera $\beta^{\star}$ l'estimateur des moindres carrés.
\paragraph{Question 2 :}

Voir code.

On obtient une RMSE de 3.85, sachant que la variance de y est 90 à peu près.

\section{Régression pénalisée : le lasso}

\paragraph{Question 3 :}
\begin{proof}
On ne peut pas utiliser la régression par moindres carrés si le nombre de variables est supérieur au nombre d'observations
\\X est une matrice de dimensions (n,p)
\\Donc $X^{T}X$ est une matrice carrée de dimension p
\\On a $rg(X^{T}X) = rg(X)$
\\Donc $rg(X^{T}X) \leq min(n,p)$
\\Par hypothèse, le nombre de variables est plus grand que le nombre d'observations, soit $p > n$
\\Donc $rg(X^{T}X) < p$
\\Or $X^{T}X$ est une matrice de taille p
\\Donc $X^{T}X$ n'est pas inversible
\\Donc on ne peut pas utiliser la méthode précédente.
\\La matrice $X^{T}X$ n'est plus inversible car son rang est inférieur au minimum des dimensions de X.
\end{proof}

\paragraph{Question 4 :}

\begin{align}
\intertext{Sous l'hypothèse de p=1, on résoud le problème d'optimisation:}
& \underset{\beta}{min}(y-x\beta)^2 + \lambda|\beta| \notag
\intertext{A partir de l'expression (1), on a $\hat{\beta} = \frac{y}{x}$}
\intertext{On injecte $\hat{\beta}$ dans le problème d'optimisation et après simplification:}
& \underset{\beta}{min} \frac{x^{2}\beta^{2}}{2} - x^{2} \beta \hat{\beta} + \lambda|\beta| \notag
\intertext{On remarque que si $\hat{\beta}<0$ Alors forcément $\beta^{lasso}<0$}
\intertext{De même si $\hat{\beta}>0$ Alors forcément $\beta^{lasso}>0$}
\intertext{On va traîter les deux cas}
\intertext{Si $\hat{\beta}>0$, on résoud:}
& \underset{\beta}{min} \frac{x^{2}\beta^{2}}{2} - x^{2} \beta \hat{\beta} + \lambda\beta \notag
\intertext{On obtient $\beta^{lasso} = \hat{\beta} - \frac{\lambda}{x^{2}}$}
\intertext{Or $\beta^{lasso} $ forcément positif, donc $\beta^{lasso} = max(0, \hat{\beta} - \frac{\lambda}{x^{2}})$}
\intertext{Si $\hat{\beta}<0$, à l'aide d'un raisonnement similaire $\beta^{lasso} = max(0, \hat{\beta} + \frac{\lambda}{x^{2}})$}
\intertext{On en déduit que $\beta^{lasso} = sg(\hat{\beta}) (|\hat{\beta}| - t )_{+}$ avec $t = \frac{\lambda}{x^{2}}$}
\end{align}



Le seuillage doux permet de regler l'intervalle sur lequel on veut estimer $\beta^{\star}$. Ainsi, on choisit $\lambda$, si notre estimateur $\beta^{\star}$ est compris dans l'intervalle $[-\lambda,\lambda]$ il est ramené à 0. On ne retient que les valeurs de $\beta^{\star}$ en dehors de cet intervalle (même principe qu'un filtre).
\\L'estimateur $\hat{\beta}^{lasso}$ est nul pour tout les $\beta^{\star}$ tels que $\beta^{\star} - t < 0$. Donc cela induit une estimation nulle plus souvent que l'estimateur des moindres carrés (qui se produit seulement lorsque $\beta^{\star} = 0$)

\paragraph{Question 5:}
Dans la régression pénalisée, on pénalise les valeurs de $\beta$ pour limiter leur valeurs possibles, dans le cas du lasso par exemple, toutes les estimations par moindres carrées inférieures à t sont ramenées  zeros, l'intervalle d'estimation est $\left\lbrace0\right\rbrace$U$[t,\infty[$. Dans le cas de la régression Ridge par exemple, on pénalise les coefficients trop grands.
\\ L'estimateur Lasso est adapté à notre problème car nous allons pouvoir réduire la taille de l'espace des paramètres jusqu'à ce que le nombre de coefficients de beta non nuls soit égal à n le nombre d'observations, en changeant la valeur de $\lambda$. Plus celui-ci est grand, plus le nombre de paramètres nuls sera grand.



\paragraph{Question 6 :}
Quelles sont les avantages du lasso sur la pénalisation ` 0 ?
Même question avec la pénalisation Ridge

La pénalisation Ridge a tendance à pénaliser les valeurs dans $\beta$ trop grandes, mais n'a pas tendance à maximiser le nombre de paramètres nuls, contrairement à la pénalisation l0.

\paragraph{Question 7 :}
A implémenter

\paragraph{Question 8 :}
Le parametre $lambda$ permet de régler le nombre de coefficients nuls dans $\beta^{lasso}$.

\paragraph{Question 9 :}
Dans la méthode de cross validation, on divise le dataset en trois parties: 
Train Set, Validation Set, et Test Set.
On entraine le modèle sur le train set puis on estime la qualité de la prédiction sur le validation set pour plusieurs valeurs de $\lambda$, on choisit le lambda le plus performant sur le validation set. Esuite on estime le modèle avec cette valeur de $\lambda$ sur le train set comprenant le train set précédent et le validation set précédent.
\paragraph{Question 10 :}
A implémenter

Combien d’éléments de $\beta$ estimez vous non-nuls ? Evaluez l’erreur de votre modèle. Trouvez
de bonnes mesures pour évaluer votre méthode



\end{document}
